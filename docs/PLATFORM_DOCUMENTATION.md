# ðŸ—ï¸ Platform Architecture Documentation

## System Overview

The **Vibe-Coding Consensus Engine** (also known as "The Qubic") is a multi-LLM orchestration platform that achieves consensus through parallel model execution, peer review, and synthesis. The system is designed to reduce hallucinations and provide more reliable AI responses by leveraging the collective intelligence of multiple language models.

---

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Loading Page    â”‚  â”‚   Main Page      â”‚  â”‚  React Frontend  â”‚  â”‚
â”‚  â”‚  (Vanilla HTML)  â”‚  â”‚  (Vanilla HTML)  â”‚  â”‚  (Vite/React)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                     â”‚                     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚                     â”‚
            â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BACKEND (FastAPI)                            â”‚
â”‚  Port: 8000                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   Routes    â”‚  â”‚   Engine    â”‚  â”‚  Providers  â”‚                  â”‚
â”‚  â”‚  /run       â”‚â”€â”€â”‚  Graph      â”‚â”€â”€â”‚  OpenAI     â”‚                  â”‚
â”‚  â”‚  /settings  â”‚  â”‚  Execution  â”‚  â”‚  OpenRouter â”‚                  â”‚
â”‚  â”‚  /conv...   â”‚  â”‚             â”‚  â”‚  Groq       â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTERNAL LLM PROVIDERS                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  OpenAI   â”‚  â”‚  Anthropicâ”‚  â”‚  Google   â”‚  â”‚   Meta    â”‚        â”‚
â”‚  â”‚  GPT-4o   â”‚  â”‚  Claude   â”‚  â”‚  Gemini   â”‚  â”‚  Llama    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## The 8-Layer Consensus Graph

The core of the system is an 8-layer processing graph that transforms raw user queries into authoritative consensus answers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSENSUS GRAPH PIPELINE                          â”‚
â”‚                                                                      â”‚
â”‚  Layer 1: NORMALIZATION                                              â”‚
â”‚  â”œâ”€ LLM analyzes user query                                         â”‚
â”‚  â”œâ”€ Extracts: intent, domain, constraints                           â”‚
â”‚  â””â”€ Produces: NormalizedPrompt                                      â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 2: CONSTRAINT LOCKING                                         â”‚
â”‚  â”œâ”€ Merges explicit + inferred constraints                          â”‚
â”‚  â”œâ”€ Generates cryptographic hash                                    â”‚
â”‚  â””â”€ Produces: LockedContext                                         â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 3: PARALLEL EXECUTION                                         â”‚
â”‚  â”œâ”€ Queries 1-4 models concurrently                                 â”‚
â”‚  â”œâ”€ Uses AsyncIO for parallelism                                    â”‚
â”‚  â””â”€ Produces: List[ModelResponse]                                   â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 4: CLAIM EXTRACTION                                           â”‚
â”‚  â”œâ”€ LLM extracts atomic, testable claims                            â”‚
â”‚  â”œâ”€ Splits compound statements                                      â”‚
â”‚  â””â”€ Produces: List[ClaimsResponse]                                  â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 4.5: PEER REVIEW                                              â”‚
â”‚  â”œâ”€ Models review each other anonymously                            â”‚
â”‚  â”œâ”€ Scores: accuracy, insight, constraint_adherence                 â”‚
â”‚  â””â”€ Produces: List[PeerReview]                                      â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 5: AGREEMENT DETECTION                                        â”‚
â”‚  â”œâ”€ Groups similar claims into clusters                             â”‚
â”‚  â”œâ”€ Identifies supporting/conflicting models                        â”‚
â”‚  â””â”€ Produces: List[ClaimCluster]                                    â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 6: CONFIDENCE SCORING                                         â”‚
â”‚  â”œâ”€ Calculates scores based on agreement                            â”‚
â”‚  â”œâ”€ Incorporates peer review scores                                 â”‚
â”‚  â””â”€ Produces: List[ScoredCluster]                                   â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 7: SYNTHESIS (Chairman)                                       â”‚
â”‚  â”œâ”€ Chairman LLM synthesizes final answer                           â”‚
â”‚  â”œâ”€ Emphasizes high-confidence conclusions                          â”‚
â”‚  â””â”€ Produces: FinalConsensus                                        â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚  Layer 8: PERSISTENCE                                                â”‚
â”‚  â”œâ”€ Saves conversation to JSON                                      â”‚
â”‚  â””â”€ Returns: GraphState with conversation_id                        â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Directory Structure

```
test-llm-council/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app, routes
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ settings.py      # Runtime key storage
â”‚   â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â”‚   â”œâ”€â”€ graph.py         # AntigravityEngine orchestrator
â”‚   â”‚   â”‚   â”œâ”€â”€ normalization.py # Layer 1-2 (prompt analysis)
â”‚   â”‚   â”‚   â”œâ”€â”€ execution.py     # Layer 3-4.5 (parallel LLM calls)
â”‚   â”‚   â”‚   â”œâ”€â”€ synthesis.py     # Layer 5-7 (agreement, scoring)
â”‚   â”‚   â”‚   â”œâ”€â”€ persistence.py   # Layer 8 (save/load conversations)
â”‚   â”‚   â”‚   â”œâ”€â”€ providers.py     # LLM provider adapters
â”‚   â”‚   â”‚   â””â”€â”€ llm.py           # Provider context management
â”‚   â”‚   â”œâ”€â”€ nodes/               # (Legacy, deprecated)
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ logger.py        # Logging utilities
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ loadingpage.html     # Landing/loading page
â”‚   â”‚   â””â”€â”€ mainpage.html        # Main application UI
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ conversations/       # Saved conversation JSON files
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_api.py          # API endpoint tests
â”‚   â”‚   â””â”€â”€ test_logic.py        # Core logic tests
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ pytest.ini
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/                    # React/Vite frontend (alternative UI)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ConsensusView.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ InputSection.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ NodeInspector.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ PeerReviewPanel.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SettingsModal.jsx
â”‚   â”‚   â”‚   â””â”€â”€ HistoryPanel.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ API_DOCUMENTATION.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Data Flow

### Request Flow

1. **User Input** â†’ User enters query in the UI
2. **API Call** â†’ Frontend POSTs to `/run` with prompt and model_count
3. **Normalization** â†’ LLM analyzes intent, domain, constraints
4. **Constraint Locking** â†’ Creates immutable context with hash
5. **Parallel Execution** â†’ Queries 1-4 LLMs concurrently
6. **Claim Extraction** â†’ LLM extracts atomic claims from responses
7. **Peer Review** â†’ Models review and score each other
8. **Agreement Detection** â†’ Groups claims into topic clusters
9. **Confidence Scoring** â†’ Calculates confidence per cluster
10. **Synthesis** â†’ Chairman LLM produces final answer
11. **Persistence** â†’ Saves to JSON file
12. **Response** â†’ Returns complete GraphState to frontend

### State Object (GraphState)

```python
class GraphState:
    raw_input: str                    # Original user query
    conversation_id: Optional[str]    # UUID after save
    normalized: NormalizedPrompt      # Layer 1 output
    locked_context: LockedContext     # Layer 2 output
    model_responses: List[ModelResponse]  # Layer 3 output
    all_claims: List[ClaimsResponse]  # Layer 4 output
    peer_reviews: List[PeerReview]    # Layer 4.5 output
    agreement_clusters: List[ClaimCluster]  # Layer 5 output
    scored_clusters: List[ScoredCluster]    # Layer 6 output
    consensus: FinalConsensus         # Layer 7 output
    errors: List[str]                 # Any errors during processing
```

---

## Provider System

The system uses a **Universal API Key** approach with automatic provider detection.

### Provider Detection Logic

```python
Key Prefix â†’ Provider
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sk-or-v1-  â†’ OpenRouter
gsk_       â†’ Groq
sk-ant-    â†’ Anthropic
sk-        â†’ OpenAI (default)
AIza       â†’ Google (Gemini)
```

### Provider Adapters

Each provider implements the `LLMProvider` abstract class:

```python
class LLMProvider(ABC):
    @property
    def provider_id(self) -> str: ...
    @property
    def name(self) -> str: ...
    def get_client(self, api_key: str) -> AsyncOpenAI: ...
    def get_default_models(self) -> List[Dict[str, str]]: ...
    def get_capabilities(self) -> List[str]: ...
```

### Available Models by Provider

| Provider | Models |
|----------|--------|
| OpenRouter | GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash |
| Groq | Llama 3.3 70B, Llama 3 70B, Mixtral 8x7B |
| OpenAI | GPT-4o, GPT-4 Turbo, GPT-3.5 Turbo |

---

## Frontend Architecture

### Static HTML Frontend (Primary)

Located in `backend/static/`, served by FastAPI's `StaticFiles`:

- **loadingpage.html**: Animated loading screen with 3D rotating cube
- **mainpage.html**: Main application with:
  - Query input field
  - Sidebar for council configuration
  - Model selection (1-4 models)
  - API key management
  - Results display with confidence scoring
  - Animated geometric background (Canvas)

### React Frontend (Alternative)

Located in `frontend/`, built with Vite:

- Modern component-based architecture
- Uses axios for API calls
- React Markdown for rendering responses
- Lucide icons for UI elements

---

## Configuration

### Environment Variables

Create `.env` file in `backend/` directory:

```env
# OpenRouter API Key (recommended)
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# Groq API Key (optional, for fast inference)
GROQ_API_KEY=gsk_your-key-here
```

### Runtime Configuration

API keys can also be set at runtime via the `/settings/keys` endpoint or the UI sidebar.

---

## Running the Application

### Backend

```bash
cd backend
pip install -r requirements.txt
python -m uvicorn app.main:app --reload --port 8000
```

### Frontend (React - optional)

```bash
cd frontend
npm install
npm run dev
```

### URLs

- **Backend + Static UI**: http://localhost:8000
- **API Status**: http://localhost:8000/api/status
- **Main Page**: http://localhost:8000/mainpage
- **React Frontend**: http://localhost:5173 (if running separately)

---

## Testing

### Run Tests

```bash
cd backend
pip install pytest pytest-asyncio
python -m pytest tests/ -v
```

### Test Coverage

- **test_api.py**: API endpoint tests
  - Health check endpoints
  - Consensus query endpoints
  - Settings/keys endpoints
  - Conversation endpoints

- **test_logic.py**: Core logic tests
  - Prompt normalization
  - Constraint locking
  - Agreement detection
  - Hash consistency

---

## Key Design Decisions

1. **All LLM calls use OpenAI SDK**: Even for non-OpenAI providers, we use the OpenAI Python SDK with custom base URLs for compatibility.

2. **Fallback behavior**: If no API key is provided or LLM calls fail, the system returns sensible defaults rather than erroring.

3. **Anonymous peer review**: Models review each other without knowing which model produced which response.

4. **Cryptographic constraint hashing**: Ensures constraint immutability throughout the pipeline.

5. **Topic-based clustering**: Uses keyword matching for claim grouping (simple but effective for MVP).

6. **Chairman synthesis**: A designated model (usually GPT-4o) synthesizes the final answer from all inputs.

---

## Extending the System

### Adding a New Provider

1. Create a class extending `LLMProvider` in `providers.py`
2. Add detection pattern to `ProviderFactory.detect_provider()`
3. Add adapter instantiation to `ProviderFactory.get_adapter()`

### Adding New Graph Layers

1. Create a new function in the appropriate engine module
2. Add output type to `models.py`
3. Add field to `GraphState`
4. Call the function in `graph.py`'s `run()` method

### Modifying Confidence Scoring

Edit `score_clusters()` in `synthesis.py` to adjust:
- Base scores
- Agreement bonuses
- Conflict penalties
- Peer review weight

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| "No API key provided" | Set key via UI sidebar or `/settings/keys` |
| Slow responses | Reduce `model_count` or use Groq for speed |
| Empty consensus | Check if models are responding (view individual responses) |
| Tests failing | Ensure `pytest-asyncio` is installed |

---

## Contributing

1. Fork the repository
2. Create a feature branch
3. Write tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

---

## License

MIT License - See LICENSE file for details.
